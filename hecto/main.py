import os
import ssl
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau

ssl._create_default_https_context = ssl._create_unverified_context

# ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from module.data_set import data_split, ImageFolderWithPaths
from module.train import train_model
from module.utils import (
    evaluate,
    save_softmax_submission,
    #plot_confusion_matrix,
    analyze_noisy_samples_with_filenames,
    TestImageDataset
)
from module.resNet_18 import get_resnet18_model

def load_noise_list(noise_txt='noise_list.txt'):
    if not os.path.exists(noise_txt):
        print("ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„° ì‚¬ìš©.")
        return set()
    with open(noise_txt, 'r') as f:
        noise_set = set(line.strip() for line in f)
    return noise_set

def filter_noise_from_imagefolder(dataset, noise_set):
    original_len = len(dataset.samples)
    dataset.samples = [s for s in dataset.samples if s[0] not in noise_set]
    print(f"ë…¸ì´ì¦ˆ {original_len - len(dataset.samples)}ì¥ ì œì™¸, ìµœì¢… í•™ìŠµìš© {len(dataset.samples)}ì¥")
    return dataset

if __name__ == "__main__":
    print("ëª¨ë“ˆì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    # 1. Transform ì •ì˜
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),
        transforms.ColorJitter(brightness=0.3, contrast=0.3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),
    ])
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),
    ])

    # 2. ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    noise_set = load_noise_list('noise_list.txt')

    # 3. í•™ìŠµ ë°ì´í„° ImageFolder + ë…¸ì´ì¦ˆ ì œì™¸
    train_path = 'data/train'
    dataset = datasets.ImageFolder(root=train_path, transform=train_transform)
    dataset = filter_noise_from_imagefolder(dataset, noise_set)
    class_names = dataset.classes

    # 4. train/val split
    train_loader, val_loader = data_split(dataset, class_names, visualize=False)
    val_loader.dataset.dataset.transform = val_transform

    # 5. í…ŒìŠ¤íŠ¸ì…‹ ê²½ë¡œ
    test_path = 'data/test'

    # 6. ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    model_list = {
        "ResNet18": lambda: get_resnet18_model(num_classes=len(class_names), pretrained=True)
    }

    for model_name, model_fn in model_list.items():
        print(f"\nğŸ“Œ Training: {model_name}")

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model_fn().to(device)

        cce = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
        save_path = f"{model_name}_best_model.pt"
        epoch = 10

        # 7. í•™ìŠµ
        train_model(model, cce, optimizer, train_loader, val_loader, device, epoch, scheduler, save_path=save_path)

        # 8. ëª¨ë¸ ë¡œë“œ ë° í‰ê°€
        model.load_state_dict(torch.load(save_path))
        val_loss, val_acc = evaluate(model, val_loader, device, cce)
        print(f"âœ… {model_name} â†’ Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%")

        # 9. í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡
        output_csv = f"{model_name}_submission.csv"
        save_softmax_submission(model, test_path, val_transform, device, class_names, output_csv=output_csv)

        # 10. (ì„ íƒ) ë…¸ì´ì¦ˆ ìƒ˜í”Œ ë¶„ì„ - íŒŒì¼ëª… í¬í•¨
        # ë¶„ì„ ì‹œì—ëŠ” íŒŒì¼ëª… ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ImageFolderWithPathsì™€ DataLoader ì‚¬ìš©!
        analysis_dataset = ImageFolderWithPaths(root=train_path, transform=val_transform)
        analysis_dataset = filter_noise_from_imagefolder(analysis_dataset, noise_set)
        analysis_loader = DataLoader(analysis_dataset, batch_size=32, shuffle=False)

        noisy_df = analyze_noisy_samples_with_filenames(
            model,
            analysis_loader,
            device,
            class_names,
            out_dir=f"{model_name}_noisy_samples",
            top_k=50
        )

        # í˜¼ë™í–‰ë ¬ ë“± ë‹¤ë¥¸ ë¶„ì„ì€ í•„ìš”ì‹œ ì£¼ì„ í•´ì œ!
        # plot_confusion_matrix(model, val_loader, device, class_names, save_path=f"{model_name}_confusion_matrix.png")
